<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>dtIngest - Unified Batch and Streaming Ingestion - DataTorrent Documentation</title>
  

  <link rel="shortcut icon" href="../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "dtIngest - Unified Batch and Streaming Ingestion";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script>
  <script src="../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> DataTorrent Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">DataTorrent RTS</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Demos</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../demo_videos/">Videos</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Writing Applications</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../demo_sales/">Sales Dimensions</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Platform</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../dtassemble/">dtAssemble - Graphical Application Builder</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../dtdashboard/">dtDashboard - Application Data Visualization</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">dtIngest - Unified Batch and Streaming Ingestion</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#dtingest-unified-streaming-batch-data-ingestion-for-hadoop">dtIngest - Unified Streaming &amp; Batch Data Ingestion for Hadoop</a></li>
                
                    <li><a class="toctree-l4" href="#key-features">Key Features</a></li>
                
                    <li><a class="toctree-l4" href="#sample-use-cases-for-dtingest">Sample Use Cases for dtIngest</a></li>
                
                    <li><a class="toctree-l4" href="#using-dtingest">Using dtIngest</a></li>
                
            
            </ul>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Apache Apex</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../apex_core/">Apache Apex Core</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../apex_malhar/">Apache Apex Malhar</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Sandbox</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../sandbox/">Sandbox Demos and Services</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../additional_docs/">Additinal Docs</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">DataTorrent Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Platform &raquo;</li>
        
      
    
    <li>dtIngest - Unified Batch and Streaming Ingestion</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="dtingest-unified-streaming-batch-data-ingestion-for-hadoop"><strong>dtIngest</strong> - Unified Streaming &amp; Batch Data Ingestion for Hadoop</h1>
<p>The Hadoop data lake is only as good as the data in the lake. Given the variety of data sources that need to pump data into Hadoop, customers often need to set-up one-off data ingestion jobs. These one-off jobs copy files using FTP &amp; NFS mounts or try to use standalone tools like ‘distCP’ to move data in and out of hadoop. Since these jobs stitch together multiple tools, they encounter several problems around manageability, failure recovery, ability to scale to handle data skews. The DataTorrent data ingestion &amp; distribution application is </p>
<h2 id="key-features">Key Features</h2>
<p>DataTorrent dtIngest makes configuring and running Hadoop data ingestion and data extraction a point-and-click process and includes enterprise-grade features not available in the market today:</p>
<ul>
<li><strong>Apache 2.0 open-source Project Apex based</strong> – Built on <a href="https://www.datatorrent.com/project-apex/">Project Apex</a>, dtIngest is a native YARN application. It is completely fault tolerant so unlike other tools like distCP, it can ‘resume’ file ingest on failure. It is horizontally scalable and support extremely high throughput and low latency data ingest.</li>
<li><strong>Simple to use &amp; manage</strong> – A point-and-click application user interface makes it easy to configure, save &amp; launch multiple data ingestion &amp; distribution pipelines. Centralized management, visibility, monitoring and summary logs</li>
<li><strong>Batch as well as stream data</strong> -  dtIngest supports moving data between NFS, (S)FTP, HDFS, AWS S3n, Kafka and JMS so you can use one platform to exchange data across multiple endpoints.</li>
<li><strong>HDFS small file ingest using ‘compaction’</strong> –  Configurable automatic compaction of small files into large files during ingest into HDFS. Helps prevent running out of HDFS namenode namespace</li>
<li><strong>Secure and efficient data movement</strong> – Supports compression and encryption during ingest. Works with kerberos enabled secure Hadoop clusters.</li>
<li><strong>Runs in any Hadoop 2.0 Cluster</strong> -  <a href="https://www.datatorrent.com/product/supported-hadoop-distributions/">Certified</a> to run across all major Hadoop distros in physical, virtual or in the cloud deployments.</li>
</ul>
<h2 id="sample-use-cases-for-dtingest">Sample Use Cases for dtIngest</h2>
<ul>
<li>Bulk as well as incremental data loading of large as well as small files into Hadoop</li>
<li>Distributing cleansed/normalized data from Hadoop</li>
<li>Ingesting change data from Kafka/JMS into Hadoop</li>
<li>Selectively replicating data from one Hadoop cluster to another</li>
<li>Ingest streaming event data into hadoop</li>
<li>Replaying log data stored in HDFS as Kafka/JMS streams</li>
</ul>
<h2 id="using-dtingest">Using dtIngest</h2>
<ol>
<li>dtIngest is free to use with all three DataTorrent <a href="https://www.datatorrent.com/product/edition-overview/">editions</a> It is available under the application package named ‘dtIngest’</li>
<li>Navigate the the ‘Develop’ tab ‘Launch’ to launch a new ingestion application</li>
<li>
<p>dtIngest is designed to move data between any of the supported data sources &amp; destinations. Just pick &amp; configure the ones you need. Also, when moving data from file based sources, you can choose for the pipeline to run ‘one time’ or to continuously poll the input directories to look for files that match the filtering criteria</p>
<p><img alt="" src="../images/dtingest/image000.png" /></p>
</li>
<li>
<p>When copying data to ‘file’ based destinations several useful options are available. These include being able to keep source directory structure, overwrite files on destination &amp; automatically creating a hourly directory structure to track when data was written at the output. </p>
<p><img alt="" src="../images/dtingest/image001.png" /></p>
</li>
<li>
<p>Before saving the data into the destination location, it can be compressed as well as encrypted.</p>
<p><img alt="" src="../images/dtingest/image002.png" /></p>
</li>
<li>
<p>When ingesting data into HDFS, small files can be combined into ‘large’ files to help work around the namenode namespace restrictions.  </p>
<p><img alt="" src="../images/dtingest/image003.png" /></p>
</li>
<li>
<p>After configuring the ingestion application hit the ‘Launch’ button to launch the ingestion application. All the requisite connectors like Kafka, JMS, HDFS etc. will be automatically instantiated and the right operators for compression, encryption etc will be inserted into the application to generate the ‘DAG’.<br />
The screenshot below shows sample of a DAG that is generated to read files from FTP and ingest them into HDFS</p>
<p><img alt="" src="../images/dtingest/image004.png" /></p>
</li>
<li>
<p>Once the application is launched, you can navigate the the ‘Monitor’ tab and find your application instance in the list. Clicking on the application instance will give you complete visibility into the metrics of the application. </p>
</li>
<li>Summary logs from the application will be available under the ‘summary’ folder in the HDFS directory dedicated for the application. On the sandbox, you can substitute the <code>&lt;user-id&gt;</code> with <code>dtadmin</code><pre><code>/user/&lt;user-ID&gt;/datatorrents/apps/APP_ID/summary
</code></pre>
</li>
</ol>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../apex_core/" class="btn btn-neutral float-right" title="Apache Apex Core"/>Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../dtdashboard/" class="btn btn-neutral" title="dtDashboard - Application Data Visualization"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../dtdashboard/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../apex_core/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
